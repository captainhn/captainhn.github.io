<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="回归算法是一种通过最小化预测值与实际结果值之间的差距，而得到输入特征之间的最佳组合方式的一类算法。对于连续值预测有线性回归等，而对于离散值/类别预测，我们也可以把逻辑回归等也视作回归算法的一种。　　线性回归与逻辑回归是机器学习中比较基础又很常用的内容。线性回归主要用来解决连续值预测的问题，逻辑回归用来解决分类的问题，输出的属于某个类别的概率，工业界经常会用逻辑回归来做排序。在SVM、GBDT、Ad">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归与逻辑回归">
<meta property="og:url" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/index.html">
<meta property="og:site_name" content="CHN&#39;s Blog">
<meta property="og:description" content="回归算法是一种通过最小化预测值与实际结果值之间的差距，而得到输入特征之间的最佳组合方式的一类算法。对于连续值预测有线性回归等，而对于离散值/类别预测，我们也可以把逻辑回归等也视作回归算法的一种。　　线性回归与逻辑回归是机器学习中比较基础又很常用的内容。线性回归主要用来解决连续值预测的问题，逻辑回归用来解决分类的问题，输出的属于某个类别的概率，工业界经常会用逻辑回归来做排序。在SVM、GBDT、Ad">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/1.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/2.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/3.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/4.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/5.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/6.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/7.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/8.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/9.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/10.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/11.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/12.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/13.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/14.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/15.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/16.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/17.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/18.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/19.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/20.png">
<meta property="og:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/21.png">
<meta property="og:updated_time" content="2018-06-01T09:25:51.779Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="线性回归与逻辑回归">
<meta name="twitter:description" content="回归算法是一种通过最小化预测值与实际结果值之间的差距，而得到输入特征之间的最佳组合方式的一类算法。对于连续值预测有线性回归等，而对于离散值/类别预测，我们也可以把逻辑回归等也视作回归算法的一种。　　线性回归与逻辑回归是机器学习中比较基础又很常用的内容。线性回归主要用来解决连续值预测的问题，逻辑回归用来解决分类的问题，输出的属于某个类别的概率，工业界经常会用逻辑回归来做排序。在SVM、GBDT、Ad">
<meta name="twitter:image" content="http://yoursite.com/2018/05/31/线性回归与逻辑回归/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/05/31/线性回归与逻辑回归/"/>





  <title>线性回归与逻辑回归 | CHN's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">CHN's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">What makes your heart sing?</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/31/线性回归与逻辑回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Haonan Cui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/img/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CHN's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">线性回归与逻辑回归</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-31T17:05:10+08:00">
                2018-05-31
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/31/线性回归与逻辑回归/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/05/31/线性回归与逻辑回归/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>回归算法是一种通过最小化预测值与实际结果值之间的差距，而得到输入特征之间的最佳组合方式的一类算法。对于连续值预测有线性回归等，而对于离散值/类别预测，我们也可以把逻辑回归等也视作回归算法的一种。<br>　　线性回归与逻辑回归是机器学习中比较基础又很常用的内容。线性回归主要用来解决连续值预测的问题，逻辑回归用来解决分类的问题，输出的属于某个类别的概率，工业界经常会用逻辑回归来做排序。在SVM、GBDT、AdaBoost算法中都有涉及逻辑回归，回归中的损失函数、梯度下降、过拟合等知识点也经常是面试考察的基础问题。因此很重要的两个内容，需要仔细体会~<br><a id="more"></a></p>
<h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p><img src="/2018/05/31/线性回归与逻辑回归/1.png" width="80%" height="80%" align="center/"></p>
<p>线性模型（Linear Model）试图学得一个通过属性的线性组合来进行预测的函数：<br>\begin{equation}<br>f(x)=w_{1}x_{1}+w_{2}x_{2}+…+w_{d}x_{d}+b<br>\end{equation}<br>向量表达形式：</p>
<p>\begin{equation}<br>f(x)=w^{T}x+b<br>\end{equation}<br><strong>特点：</strong>简单、基本、可解释性好</p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="线性回归算法"><a href="#线性回归算法" class="headerlink" title="线性回归算法"></a>线性回归算法</h4><p>在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。<br><strong>优点：</strong>结果易于理解，计算上不复杂<br><strong>缺点：</strong>对非线性的数据拟合不好<br><strong>适应的数据类型：</strong>数值型和标称型数据</p>
<ul>
<li>有监督学习，学习样本有标签</li>
<li>输出结果是连续性变量</li>
<li>需要学习映射</li>
<li>假定输入x和输出y之间有线性相关关系$f:x-&gt;y$<h4 id="房屋价格预测"><a href="#房屋价格预测" class="headerlink" title="房屋价格预测"></a>房屋价格预测</h4></li>
<li>一元<br><img src="/2018/05/31/线性回归与逻辑回归/2.png" width="80%" height="80%" align="center/"></li>
<li>多元<br><img src="/2018/05/31/线性回归与逻辑回归/3.png" width="80%" height="80%" align="center/"></li>
</ul>
<p><img src="/2018/05/31/线性回归与逻辑回归/4.png" width="80%" height="80%" align="center/"></p>
<p><img src="/2018/05/31/线性回归与逻辑回归/5.png" width="80%" height="80%" align="center/"></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>我们希望找到最好的权重/参数$$\theta =[ \theta_{0},\theta_{1},….\theta_{n} ]$$<br><strong>如何衡量“最好”？</strong></p>
<p><strong>损失函数：</strong>是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数。更通俗地说，损失函数用来衡量参数选择的准确性。函数定义为：</p>
<p>\begin{equation}<br>J(\theta_{0},\theta_{1}…\theta_{n})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{i})-y^{i})^2<br>\end{equation}</p>
<p>这个公式计算的是线性回归分析的值与实际值的距离平方的平均值。显然，损失函数得到的值越小，损失也就越小。</p>
<h4 id="最小化损失函数"><a href="#最小化损失函数" class="headerlink" title="最小化损失函数"></a>最小化损失函数</h4><p>均方误差损失是个凸函数：</p>
<p><img src="/2018/05/31/线性回归与逻辑回归/6.png" width="80%" height="80%" align="center/"><br><img src="/2018/05/31/线性回归与逻辑回归/7.png" width="80%" height="80%" align="center/"></p>
<h5 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h5><p>逐步最小化损失函数的过程。如同下山的过程，找准下山方向（梯度），每次迈进一步，直至山底。如果有多个特征，对应多个参数$\theta_{i}$需要对每一个参数做一次迭代，做完以后再求损失函数。<br><strong>以一元线性回归为例：</strong><br>首先，我们有一个代价函数，假设是J(\theta_{0},\theta_{1})，我们的目标是最小化这个函数。<br>接下来的做法是：</p>
<ul>
<li>首先随机选择一个参数的组合$(\theta_{0},\theta_{1})$ ,一般设为0.</li>
<li>然后是不断改变$(\theta_{0},\theta_{1})$，并计算代价函数，【直到一个局部最小值。之所以是局部最小值，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值】【最后得到全局最优解，因为每迭代一步，都要用到训练集所有的数据】。<br>下面给出梯度下降算法的公式：<br>\begin{equation}<br>\theta_{j}=\theta_{j}-\alpha\frac{\partial}{\partial \theta_{j}}J(\theta_{0},\theta_{1}), for j = 0 and j = 1<br>\end{equation}<br>而应用梯度下降法到线性回归，则公式如下： </li>
</ul>
<p>\begin{equation}<br>\theta_{0}=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{i})-y^{i})<br>\end{equation}</p>
<p>\begin{equation}<br>\theta_{1}=\theta_{1}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{i})-y^{i})\cdot x^{i}<br>\end{equation}</p>
<p><img src="/2018/05/31/线性回归与逻辑回归/8.png" width="80%" height="80%" align="center/"><br><img src="/2018/05/31/线性回归与逻辑回归/9.png" width="80%" height="80%" align="center/"><br>学习率：上段公式中的α就是学习率。它决定了下降的节奏快慢，就像一个人下山时候步伐的快慢。α过小会导致收敛很慢，α太大有可能会导致震荡。如何选择学习率呢，目前也有好多关于学习率自适应算法的研究。工程上，一般会调用一些开源包，包含有一些自适应方法。自己做的话会选择相对较小的α，比如0.01。</p>
<p> <strong>批量梯度下降（Batch Gradient Descent，简称BGD）：</strong><br> 是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新，上述公式就是使用批量梯度下降。</p>
<ul>
<li>优点：全局最优解；易于并行实现</li>
<li>缺点：当样本数目很多时，训练过程会很慢。</li>
</ul>
<p><strong>随机梯度下降（SGD Stochastic gradientdescent ）：</strong><br> 针对BGD算法训练速度过慢的缺点，提出了SGD算法，它的具体思路是在更新每一参数时都使用一个样本来进行更新，也就是方程中的m等于1。每一次跟新参数都用一个样本，更新很多次。如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了。<br> 随机梯度下降的权重更新要比批量梯度下降的权重更新更为频繁，因为批量梯度下降在对于整个训练集的时候，权重只更新一次，而随机梯度下降的更新次数与训练集的大小是一样的，所以相对于批量梯度下降而已，随机梯度下降的收敛速度会更快一些。由于随机梯度的权重更新是基于单个的训练样本而言的，所以误差曲面不会像梯度下降那么平滑，这也使得随机梯度下降更容易的跳出小范围的局部最优解。<br> 为了通过随机梯度下降得到的结果更加准确，让训练数据以随机的方式进行训练是非常重要的，所以每次迭代都要打乱训练集以防止进入循环。在使用随机梯度下降算法的时候，我们可以根据迭代次数的变化来调整随机梯度下降算法的学习率α，如c1/(α+c2)，其中c1和c2为常数。需要注意的时候，随机梯度下降算法获取的最优解不一定是全局最优解，它会趋于全局最优解。通过使用自适应的学习率，可以更进一步的趋于全局最优解。<br> 我们还可以将随机梯度下降算法应用于在线学习系统中，通过在线学习系统，当有新数据产生的时候模型会被实时训练，而不会重置模型的权重。这种算法通常应用于海量数据和web系统中，而且这种算法还能减少存储数据的成本，我们可以在训练完成之后，保存模型，然后将数据废弃掉。</p>
<ul>
<li>优点：训练速度快</li>
<li>缺点：噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向；准确度下降，并不是全局最优；不易于并行实现。<br><strong>小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）：</strong><br>SGD相对来说要快很多，但是也有存在问题，由于单个样本的训练可能会带来很多噪声，使得SGD并不是每次迭代都向着整体最优化方向，因此在刚开始训练时可能收敛得很快，但是训练一段时间后就会变得很慢。<br>MBGD的具体思路是在更新每一参数时都使用一部分样本来进行更新，也就是方程中的m的值大于1小于所有样本的数量。为了克服上面两种方法的缺点，又同时兼顾两种方法的有点。</li>
</ul>
<p><strong>如果样本量比较小，采用批量梯度下降算法。如果样本太大，或者在线算法，使用随机梯度下降算法。在实际的一般情况下，采用小批量梯度下降算法。</strong></p>
<h5 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h5><p>牛顿法：速度快适用于小数据，大数据比较耗内存。</p>
<h4 id="过拟合和正则化"><a href="#过拟合和正则化" class="headerlink" title="过拟合和正则化"></a>过拟合和正则化</h4><p>归回与欠/过拟合：</p>
<ol>
<li>欠拟合：函数假设太简单导致无法覆盖足够的原始数据，可能造成数据预测的不准确。模型没有很好地捕捉到数据特征，不能够很好地拟合数据  </li>
<li>把样本中的一些噪声特性也学习下来了，泛化能力差<br><img src="/2018/05/31/线性回归与逻辑回归/10.png" width="80%" height="80%" align="center/"></li>
</ol>
<p>实际工业界使用的各种模型都存在过拟合的风险： </p>
<ul>
<li>更多的参数/特征，更复杂的模型，通常有更强的学习能力，但是更容易“失去控制</li>
<li>训练集中有一些噪声，并不代表全量真实数据的分布，死记硬背会丧失泛化能力。</li>
</ul>
<p><strong>过拟合解决方法：</strong></p>
<ol>
<li>减少特征个数：手工选择保留特征、模型选择的算法选择特征。</li>
<li>正则化：在原来的损失函数中加入θ的平方项，来防止波动太大。 </li>
</ol>
<p><strong>正则化：</strong>通知正则化添加参数“惩罚”，控制参数幅度 限制参数搜索空间，减小过拟合风险。</p>
<p>\begin{equation}<br>J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_{\theta}(x^{i})-y^{i})^2+\lambda \sum_{j=1}^{n}\theta_{j}^{2}]<br>\end{equation}<br>其中m是样本的数量，n为$\theta$的数量。</p>
<p><img src="/2018/05/31/线性回归与逻辑回归/11.png" width="80%" height="80%" align="center/"></p>
<h4 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h4><p>有时候样本关系不一定是线性的 如何逼近y 的衍生物？<br>比如令：<br>\begin{equation}<br>\ln y=w^{T}x +b<br>\end{equation}<br>则得到对数线性回归 (log-linearregression)<br>实际上在用$e^{w^{T}x +b}$逼近y.</p>
<ul>
<li>对线性映射的结果进行数学变换， 去逼近y值 </li>
<li>指数(exp)或者对数(log)变换处理</li>
</ul>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>####从线性回归到逻辑回归</p>
<p>与线性回归不同，逻辑回归主要用于解决分类问题，那么线性回归能不能做同样的事情呢？下面举一个例子。比如恶性肿瘤和良性肿瘤的判定。假设我们通过拟合数据得到线性回归方程和一个阈值，用阈值判定是良性还是恶性： </p>
<p><img src="/2018/05/31/线性回归与逻辑回归/12.png" width="80%" height="80%" align="center/"><br>如图，size小于某值就是良性，否则恶性。但是“噪声”对线性方程的影响特别大，会大大降低分类准确性。例如再加三个样本就可以使方程变成这样： </p>
<p><img src="/2018/05/31/线性回归与逻辑回归/13.png" width="80%" height="80%" align="center/"></p>
<p>那么，逻辑斯特回归是怎么做的呢？如果不能找到一个绝对的数值判定肿瘤的性质，就用概率的方法，预测出一个概率，比如&gt;0.5判定为恶性的。</p>
<h4 id="逻辑回归决策边界"><a href="#逻辑回归决策边界" class="headerlink" title="逻辑回归决策边界"></a>逻辑回归决策边界</h4><p><strong>在逻辑回归(LogisticRegression)里，通常我们并不拟合样本分布，而是确定决策边界</strong></p>
<p><img src="/2018/05/31/线性回归与逻辑回归/14.png" width="80%" height="80%" align="center/"><br>逻辑回归首先把样本映射到[0,1]之间的数值，这就归功于sigmoid函数，可以把任何连续的值映射到[0,1]之间，数越大越趋向于0，越小越趋近于1。Sigmoid函数公式如下： </p>
<p>\begin{equation}<br>y =\frac{1}{1+e^{-z}}<br>\end{equation}<br><img src="/2018/05/31/线性回归与逻辑回归/15.png" width="80%" height="80%" align="center/"></p>
<p>x=0的时候y对应中心点。 </p>
<ul>
<li>线性决策边界<br><img src="/2018/05/31/线性回归与逻辑回归/16.png" width="80%" height="80%" align="center/"></li>
</ul>
<p>找到一组θ，假设得到−3+x1+x2=0的直线，把样本分成两类。把(1,1)代入g函数，概率值&lt;0.5，就判定为负样本。这条直线就是判定边界.如上图。</p>
<ul>
<li>非线性决策边界</li>
</ul>
<p><img src="/2018/05/31/线性回归与逻辑回归/17.png" width="80%" height="80%" align="center/"></p>
<h4 id="逻辑回归损失函数"><a href="#逻辑回归损失函数" class="headerlink" title="逻辑回归损失函数"></a>逻辑回归损失函数</h4><ul>
<li>均方差损失（MSE）？<br>线性回归的损失函数对逻辑回归不可用，因为逻辑回归的值是0或者1，求距离平均值会是一条不断弯曲的曲线，不是理想的凸函数。</li>
</ul>
<p><img src="/2018/05/31/线性回归与逻辑回归/18.png" width="80%" height="80%" align="center/"><br>其中$h^{\theta}(x^{i})$的取值为0到1.<br>$y^{i}$取值为0或1.<br>然鹅我们希望损失函数应该为理想的凸函数</p>
<p><img src="/2018/05/31/线性回归与逻辑回归/19.png" width="80%" height="80%" align="center/"></p>
<ul>
<li>对数损失/二元交叉熵损失<br>聪明的数学家找到了一个适合逻辑回归的损失定义方法：<br>\begin{equation}<br>Cost(h_{\theta}(x),y)=\left{<br>\begin{aligned}<br>-log(h_{\theta}(x)) &amp;  &amp; if y=1 \<br>-log(1-h_{\theta}(x)) &amp;  &amp;  if y=0<br>\end{aligned}<br>\right.<br>\end{equation}</li>
</ul>
<p>其中hθ(x)是一个概率值，y=1表示正样本，y=0表示负样本。当y是正样本时，如果给定的概率特别小（预测为负样本），损失就会很大；给定的概率很大（预测为正样本），损失就会接近0。损失值的函数如图：</p>
<p><img src="/2018/05/31/线性回归与逻辑回归/20.png" width="80%" height="80%" align="center/"></p>
<p>我们将上述分段函数进行简化：</p>
<p>\begin{equation}<br>J(\theta)= \frac{1}{m}\sum_{i=1}{m}Cost(h_{\theta}(x^{i}),y^{i})<br>\end{equation}<br>简化分段函数：<br>\begin{equation}<br>J(\theta)= -\frac{1}{m}[\sum_{i=1}^{m}y^{i}logh_{\theta}(x^{i})+(1-y^{i})log(1-h_{\theta}(x^{i}))]<br>\end{equation}<br>依旧存在过拟合问题，决策边界可能抖动很厉害！！！添加正则化：</p>
<p>\begin{equation}<br>J(\theta)= -\frac{1}{m}[\sum_{i=1}^{m}y^{i}logh_{\theta}(x^{i})+(1-y^{i})log(1-h_{\theta}(x^{i}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}<br>\end{equation}</p>
<ul>
<li>如何最小化损失函数？对于凸函数，依然可以使用<strong>梯度下降</strong>求导的时候注意<strong>链式求导法</strong><h4 id="从二分类到多分类"><a href="#从二分类到多分类" class="headerlink" title="从二分类到多分类"></a>从二分类到多分类</h4></li>
</ul>
<p>刚才讲述的都是二分类的问题，那如果是多分类的问题，又该怎么做呢？其实可以套用二分类的方法，根据特征，一层层细化类别。比如下图中有三种形状：</p>
<p><img src="/2018/05/31/线性回归与逻辑回归/21.png" width="80%" height="80%" align="center/"></p>
<p>可以先用一个分类器区分“正方形”和“非正方形”，再用一个分类器对非正方形区分，得到“三角形”和“非三角形”，然后再用一个分类器区分叉。</p>
<h3 id="工程经验"><a href="#工程经验" class="headerlink" title="工程经验"></a>工程经验</h3><p>逻辑斯特回归（LR）是个比较基础的算法，在它只会有很多算法SVM/GBDT/RandomForest。复杂的算法比较难以把握，工业界更偏向于用简单的算法。</p>
<h4 id="LR-的优点以及应用"><a href="#LR-的优点以及应用" class="headerlink" title="LR 的优点以及应用"></a>LR 的优点以及应用</h4><p><strong>模型本身没有好坏之分，只有合不合适</strong></p>
<ul>
<li>LR是以概率的形式进行输出结果，而不只是0和1 的判定</li>
<li>LR 的可解释性强，可控性高</li>
<li>训练速度快，特征工程(feature engineering)之后效果赞</li>
<li>因为结果是概率，可以做排序模型 </li>
<li>添加特征非常简单… </li>
</ul>
<p><strong>应用：</strong></p>
<ul>
<li>CTR预估/推荐系统的learning to rank/各种分类场景</li>
<li>很多搜索引擎厂的广告CTR预估基线版是LR </li>
<li>电商搜索排序/广告CTR预估基线版是LR </li>
<li>新闻app的推荐和排序基线也是LR</li>
</ul>
<h4 id="样本处理"><a href="#样本处理" class="headerlink" title="样本处理"></a>样本处理</h4><ul>
<li>样本特征处理<br>离散化后用独热编码（one -hot encoding ）处理成0，1值<br>如果一定要用连续值的话，可以做scaling；  </li>
<li>处理大样本量<br>工具的话有 spark Mllib，它损失了一小部分的准确度达到速度的提升；<br>如果没有并行化平台，想做大数据就试试采样。需要注意采样数据，最好不要随机取，可以按照日期/用户/行为，来分层抽样</li>
<li>注意样本的均衡<br> 如果样本不均衡，样本充足的情况下可以做下采样——抽样，样本不足的情况下做上采样——对样本少的做重复；<br> 修改损失函数，给不同权重。比如负样本少，就可以给负样本大一点的权重；<br>采样后的predict结果，用作判定请还原。</li>
</ul>
<h4 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h4><ul>
<li>离散化优点：映射到高维空间，用linear的LR(快，且兼具更好的分割性)；稀疏化，0,1向量内积乘法运算速度快，计算结果方 便存储，容易扩展；离散化后，给线性模型带来一定的非线性；模型稳定，收敛度高，鲁棒性好；在一定程度上降低了过拟合风险 </li>
<li>通过组合特征引入个性化因素：比如uuid+tag </li>
<li>注意特征的频度： 区分特征重要度，可以用重要特征产出层次判定模型 <h4 id="模型调优"><a href="#模型调优" class="headerlink" title="模型调优"></a>模型调优</h4></li>
<li>选择合适的正则化：L2准确度高，训练时间长；L1可以做一定的特征选择，适合大量数据 </li>
<li>收敛阈值e，控制迭代轮数 </li>
<li>样本不均匀时调整loss function,给不同权重 </li>
<li>Bagging或其他方式的模型融合 </li>
<li>选择最优化算法：liblinear、sag、newton-cg等</li>
</ul>
<h4 id="工具包"><a href="#工具包" class="headerlink" title="工具包"></a>工具包</h4><p>Liblinear |   Spark Mllib  |  Scikit-learn</p>
<ul>
<li>Liblinear:<a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a></li>
<li>Spark:<a href="http://spark.apache.org/docs/latest/mllib-linear-methods.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/mllib-linear-methods.html</a></li>
<li>Sciki-learn:<a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></li>
</ul>
<h3 id="实战案例"><a href="#实战案例" class="headerlink" title="实战案例"></a>实战案例</h3><h4 id="线性回归-1"><a href="#线性回归-1" class="headerlink" title="线性回归"></a>线性回归</h4><ul>
<li>熟悉numpy</li>
<li>单变量线性回归</li>
<li>梯度下降<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">#导入需要的库</span><br><span class="line">import pandas as pd </span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.mpl_toolkits.mplot3d import axes3d</span><br><span class="line"></span><br><span class="line">#熟悉numpy</span><br><span class="line">def warmupexercise():</span><br><span class="line">    return(np.identity(5))</span><br><span class="line"></span><br><span class="line">warmupexercise()</span><br><span class="line"></span><br><span class="line">#单变量线性回归 </span><br><span class="line">#载入数据文件</span><br><span class="line">data = np.loadtxt(&apos;linear_regression_datal.txt&apos;,delimiter=&apos;,&apos;)</span><br><span class="line">#赋值到X，y</span><br><span class="line">X = np.c_[np.ones(data.shape[0]),data[:,0]]</span><br><span class="line">y = np.c_[data[:,1]]</span><br><span class="line">#对样本进行画图</span><br><span class="line">plt.scatter(X[:,1],y,s=30,c=&apos;r&apos;，marker=&apos;x&apos;,linewidths=1)</span><br><span class="line">plt.xlim(4,24)</span><br><span class="line">plt.xlabel(&apos;Population of City in 10,000s&apos;)</span><br><span class="line">plt.ylabel(&apos;Profit in $10,000s&apos;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">#梯度下降</span><br><span class="line">#计算损失函数</span><br><span class="line">def computeCost(X,y,theta=[[0],[0]]):</span><br><span class="line">    m = y.size</span><br><span class="line">    J = 0</span><br><span class="line">    h = X.dot(theta)</span><br><span class="line">    J = 1.0/(2*m)*(np.sum(np.square(h-y))) </span><br><span class="line">    return J</span><br><span class="line">computeCost()</span><br><span class="line"></span><br><span class="line">#梯度下降</span><br><span class="line">def gradientDescent(X, y theta=[[0],[0]], alpha=0.01, num_iters=1500):</span><br><span class="line">    m = y.size</span><br><span class="line">    J_history = np.zeros(num_iters)</span><br><span class="line"></span><br><span class="line">    for iter in np.arange(num_iters):</span><br><span class="line">         h = X.dot(theta)</span><br><span class="line">         theta = theta - alpha*(1.0/m)*(X.T.dot(h-y))</span><br><span class="line">         J_history[iter] = computeCost(X,y,theta)</span><br><span class="line">    return(theta, J_history)</span><br><span class="line">#画出每一次迭代的损失函数变化</span><br><span class="line">theta, Cost_J = gradientDescent(X,y)</span><br><span class="line">print(&apos;theta:&apos;， theta.ravel())</span><br><span class="line">plt.plot(Cost_J)</span><br><span class="line">plt.ylabel(&apos;Cost J&apos;)</span><br><span class="line">plt.xlabel(&apos;Iterations&apos;)</span><br><span class="line">plt.show()</span><br><span class="line">#设置得出的直线x,y</span><br><span class="line">xx = np.arange(5,23)</span><br><span class="line">yy = theta[0]+theta[1]*xx</span><br><span class="line"># 画出我们自己写的线性回归梯度下降收敛的情况</span><br><span class="line">plt.scatter(X[:,1], y, s=30, c=&apos;r&apos;, marker=&apos;x&apos;, linewidths=1)</span><br><span class="line">plt.plot(xx,yy, label=&apos;Linear regression (Gradient descent)&apos;)</span><br><span class="line"># 和Scikit-learn中的线性回归对比一下 </span><br><span class="line">regr = LinearRegression()</span><br><span class="line">regr.fit(X[:,1].reshape(-1,1), y.ravel())</span><br><span class="line">plt.plot(xx, regr.intercept_+regr.coef_*xx, label=&apos;Linear regression (Scikit-learn GLM)&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlim(4,24)#x轴从4到24</span><br><span class="line">plt.xlabel(&apos;Population of City in 10,000s&apos;)</span><br><span class="line">plt.ylabel(&apos;Profit in $10,000s&apos;)</span><br><span class="line">plt.legend(loc=4);</span><br><span class="line"># 预测一下人口为35000和70000的城市的结果</span><br><span class="line">print(theta.T.dot([1, 3.5])*10000)</span><br><span class="line">print(theta.T.dot([1, 7])*10000)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="逻辑回归-1"><a href="#逻辑回归-1" class="headerlink" title="逻辑回归"></a>逻辑回归</h4><ul>
<li>逻辑斯特回归</li>
<li>正则化后的<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy.optimize import minimize</span><br><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">def loaddata(file, delimeter):</span><br><span class="line">    data = np.loadtxt(file, delimiter=delimeter)</span><br><span class="line">    print(&apos;dimensions&apos;:data.shape)</span><br><span class="line">    print(data[1:6,:])</span><br><span class="line">    return(data)</span><br><span class="line">def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):</span><br><span class="line"># 获得正负样本的下标(即哪些是正样本，哪些是负样本)</span><br><span class="line">    neg = data[:,2] == 0</span><br><span class="line">    pos = data[:,2] == 1</span><br><span class="line">    es = plt.gca()</span><br><span class="line">    #data[pos][:,0]:第一个方框中表示的是正样本的位置，第二个方框中的第一个数表示样本的数量，‘：’代表全部数据，第二个值代表每一行数据的第几列</span><br><span class="line">    axes.scatter(data[pos][:,0], data[pos][:,1], marker=&apos;+&apos;, c=&apos;k&apos;, s=60, linewidth=2, label=label_pos)</span><br><span class="line">    axes.scatter(data[neg][:,0], data[neg][:,1], c=&apos;y&apos;, s=60, label=label_neg)</span><br><span class="line">    axes.set_xlabel(label_x)</span><br><span class="line">    axes.set_ylabel(label_y)</span><br><span class="line">    axes.legend(frameon= True, fancybox = True)</span><br><span class="line"></span><br><span class="line"> data = loaddata(&apos;data1.txt&apos;, &apos;,&apos;)</span><br><span class="line"></span><br><span class="line">X = np.c_[np.ones((data.shape[0], 1)),data[0:2]]</span><br><span class="line">y = np.c_[data[:,2]]</span><br><span class="line"></span><br><span class="line">plotData(data, &apos;Exam 1 score&apos;, &apos;Exam 2 score&apos;, &apos;Pass&apos;, &apos;Fail&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义sigmoid函数</span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    return(1/(1+np.exp(-z)))</span><br><span class="line"></span><br><span class="line">#其实scipy包里有一个函数可以完成一样的功能:http://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html#scipy.special.expit</span><br><span class="line"></span><br><span class="line">#定义损失函数</span><br><span class="line"></span><br><span class="line">def costFunction(theta, X, y):</span><br><span class="line">    m = y.size</span><br><span class="line">    h = sigmoid(X.dot(theta))</span><br><span class="line">    #损失函数的矩阵化</span><br><span class="line">    J = -1.0*(1.0/m)*(np.log(h).T.dot(y)+np.log(1-h).T.dot(1-y))</span><br><span class="line">    #当为nan，返回无穷大</span><br><span class="line">    if np.isnan(J[0]):</span><br><span class="line">        return(np.inf)</span><br><span class="line">    return J[0]</span><br><span class="line"></span><br><span class="line">#求解梯度</span><br><span class="line">def gradient(theta, X, y):</span><br><span class="line">    m = y.size</span><br><span class="line">    h = sigmoid(X.dot(theta.reshape(-1,1)))</span><br><span class="line">    grad = (1.0/m)*X.T.dot(h-y)</span><br><span class="line">    return(grad.flatten())# a是个矩阵或者数组,a.flatten()就是把a降到一维</span><br><span class="line"></span><br><span class="line">initial_theta = np.zeros(X.shape[1])</span><br><span class="line">cost = costFunction(initial_theta, X, y)</span><br><span class="line">grad = gradient(initial_theta, X, y)</span><br><span class="line">print(&apos;Cost: \n&apos;, cost)</span><br><span class="line">print(&apos;Grad: \n&apos;, grad)</span><br><span class="line">#最小化损失函数</span><br><span class="line">res = minimize(costFunction, initial_theta, args=(X,y), jac=gradient, options=&#123;&apos;maxiter&apos;:400&#125;)</span><br><span class="line">res</span><br><span class="line">#进行预测</span><br><span class="line">def predict(theta, X, threshold=0.5):</span><br><span class="line">    p = sigmoid(X.dot(theta.T)) &gt;= threshold</span><br><span class="line">    return(p.astype(&apos;int&apos;))</span><br><span class="line"></span><br><span class="line">#咱们来看看考试1得分45，考试2得分85的同学通过概率有多高</span><br><span class="line"></span><br><span class="line">sigmoid(np.array([1, 45, 85]).dot(res.x.T))</span><br><span class="line"></span><br><span class="line">#画决策边界</span><br><span class="line">plt.scatter(45, 85, s=60, c=&apos;r&apos;, marker=&apos;v&apos;, label=&apos;(45, 85)&apos;)</span><br><span class="line">plotData(data, &apos;Exam 1 score&apos;, &apos;Exam 2 score&apos;, &apos;Admitted&apos;, &apos;Not admitted&apos;)</span><br><span class="line">x1_min, x1_max = X[:,1].min(), X[:,1].max(),</span><br><span class="line">x2_min, x2_max = X[:,2].min(), X[:,2].max(),</span><br><span class="line">xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))</span><br><span class="line">h = sigmoid(np.c_[np.ones((xx1.ravel().shape[0],1)), xx1.ravel(), xx2.ravel()].dot(res.x))</span><br><span class="line">h = h.reshape(xx1.shape)</span><br><span class="line">plt.contour(xx1, xx2, h, [0.5], linewidths=1, colors=&apos;b&apos;);</span><br><span class="line"></span><br><span class="line">#加正则化项的逻辑斯特回归</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data2 = loaddata(&apos;data2.txt&apos;, &apos;,&apos;)</span><br><span class="line"># 拿到X和y</span><br><span class="line">y = np.c_[data2[:,2]]</span><br><span class="line">X = data2[:,0:2]</span><br><span class="line"># 画个图</span><br><span class="line">plotData(data2, &apos;Microchip Test 1&apos;, &apos;Microchip Test 2&apos;, &apos;y = 1&apos;, &apos;y = 0&apos;)</span><br><span class="line">#咱们整一点多项式特征出来(最高6阶)</span><br><span class="line">poly = PolynomialFeatures(6)</span><br><span class="line">XX = poly.fit_transform(data2[:,0:2])</span><br><span class="line"># 看看形状(特征映射后x有多少维了)</span><br><span class="line">XX.shape</span><br><span class="line"># 定义向量化的损失函数，加入正则化</span><br><span class="line">def costFunctionReg(theta, reg, *args):</span><br><span class="line">    m = y.size</span><br><span class="line">    h = sigmoid(XX.dot(theta))</span><br><span class="line">    </span><br><span class="line">    J = -1.0*(1.0/m)*(np.log(h).T.dot(y)+np.log(1-h).T.dot(1-y)) + (reg/(2.0*m))*np.sum(np.square(theta[1:]))</span><br><span class="line">    </span><br><span class="line">    if np.isnan(J[0]):</span><br><span class="line">        return(np.inf)</span><br><span class="line">    return(J[0])</span><br><span class="line"></span><br><span class="line">#向量化的偏导，注意，我们另外自己加的参数 θ0 不需要被正则化</span><br><span class="line">def gradientReg(theta, reg, *args):</span><br><span class="line">    m = y.size</span><br><span class="line">    h = sigmoid(XX.dot(theta.reshape(-1,1)))</span><br><span class="line">      </span><br><span class="line">    grad = (1.0/m)*XX.T.dot(h-y) + (reg/m)*np.r_[[[0]],theta[1:].reshape(-1,1)]</span><br><span class="line">        </span><br><span class="line">    return(grad.flatten())</span><br><span class="line"></span><br><span class="line">initial_theta = np.zeros(XX.shape[1])</span><br><span class="line">costFunctionReg(initial_theta, 1, XX, y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(1,3, sharey = True, figsize=(17,5))</span><br><span class="line"></span><br><span class="line"># 决策边界，咱们分别来看看正则化系数lambda太大太小分别会出现什么情况</span><br><span class="line"># Lambda = 0 : 就是没有正则化，这样的话，就过拟合咯</span><br><span class="line"># Lambda = 1 : 这才是正确的打开方式</span><br><span class="line"># Lambda = 100 : 卧槽，正则化项太激进，导致基本就没拟合出决策边界</span><br><span class="line"></span><br><span class="line">for i, C in enumerate([0.0, 1.0, 100.0]):</span><br><span class="line">    # 最优化 costFunctionReg</span><br><span class="line">    res2 = minimize(costFunctionReg, initial_theta, args=(C, XX, y), jac=gradientReg, options=&#123;&apos;maxiter&apos;:3000&#125;)</span><br><span class="line">    </span><br><span class="line">    # 准确率</span><br><span class="line">    accuracy = 100.0*sum(predict(res2.x, XX) == y.ravel())/y.size    </span><br><span class="line"></span><br><span class="line">    # 对X,y的散列绘图</span><br><span class="line">    plotData(data2, &apos;Microchip Test 1&apos;, &apos;Microchip Test 2&apos;, &apos;y = 1&apos;, &apos;y = 0&apos;, axes.flatten()[i])</span><br><span class="line">    </span><br><span class="line">    # 画出决策边界</span><br><span class="line">    x1_min, x1_max = X[:,0].min(), X[:,0].max(),</span><br><span class="line">    x2_min, x2_max = X[:,1].min(), X[:,1].max(),</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))</span><br><span class="line">    h = sigmoid(poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(res2.x))</span><br><span class="line">    h = h.reshape(xx1.shape)</span><br><span class="line">    axes.flatten()[i].contour(xx1, xx2, h, [0.5], linewidths=1, colors=&apos;g&apos;);       </span><br><span class="line">    axes.flatten()[i].set_title(&apos;Train accuracy &#123;&#125;% with Lambda = &#123;&#125;&apos;.format(np.round(accuracy, decimals=2), C))</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/30/机器学习评价指标/" rel="next" title="机器学习评价指标">
                <i class="fa fa-chevron-left"></i> 机器学习评价指标
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/01/金融评分卡模型/" rel="prev" title="LR之金融评分卡模型 | August分享">
                LR之金融评分卡模型 | August分享 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_fav">收藏夹</a>
<a class="jiathis_button_copy">复制网址</a>
<a class="jiathis_button_email">邮件</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_qzone">QQ空间</a>
<a class="jiathis_button_tqq">腾讯微博</a>
<a class="jiathis_button_douban">豆瓣</a>
<a class="jiathis_button_share">一键分享</a>

<a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
var jiathis_config={
  data_track_clickback:true,
  summary:"",
  shortUrl:false,
  hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script>
<!-- JiaThis Button END -->
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/img/avatar.png"
                alt="Haonan Cui" />
            
              <p class="site-author-name" itemprop="name">Haonan Cui</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性模型"><span class="nav-number">1.</span> <span class="nav-text">线性模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归"><span class="nav-number">2.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归算法"><span class="nav-number">2.1.</span> <span class="nav-text">线性回归算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#房屋价格预测"><span class="nav-number">2.2.</span> <span class="nav-text">房屋价格预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数"><span class="nav-number">2.3.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最小化损失函数"><span class="nav-number">2.4.</span> <span class="nav-text">最小化损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#梯度下降"><span class="nav-number">2.4.1.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#牛顿法"><span class="nav-number">2.4.2.</span> <span class="nav-text">牛顿法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#过拟合和正则化"><span class="nav-number">2.5.</span> <span class="nav-text">过拟合和正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#广义线性模型"><span class="nav-number">2.6.</span> <span class="nav-text">广义线性模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归"><span class="nav-number">3.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归决策边界"><span class="nav-number">3.1.</span> <span class="nav-text">逻辑回归决策边界</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归损失函数"><span class="nav-number">3.2.</span> <span class="nav-text">逻辑回归损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#从二分类到多分类"><span class="nav-number">3.3.</span> <span class="nav-text">从二分类到多分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#工程经验"><span class="nav-number">4.</span> <span class="nav-text">工程经验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LR-的优点以及应用"><span class="nav-number">4.1.</span> <span class="nav-text">LR 的优点以及应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#样本处理"><span class="nav-number">4.2.</span> <span class="nav-text">样本处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征处理"><span class="nav-number">4.3.</span> <span class="nav-text">特征处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型调优"><span class="nav-number">4.4.</span> <span class="nav-text">模型调优</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#工具包"><span class="nav-number">4.5.</span> <span class="nav-text">工具包</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实战案例"><span class="nav-number">5.</span> <span class="nav-text">实战案例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归-1"><span class="nav-number">5.1.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归-1"><span class="nav-number">5.2.</span> <span class="nav-text">逻辑回归</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Haonan Cui</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>





        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'GiaasJktzc5Yp2Dh63a7eIiR-gzGzoHsz',
        appKey: '6nRQyyJGa2DwLUOqfAoISdCt',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
